{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BreastCancer_Data_using_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7VYvITb7v4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58bDuWpH7v4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Download data from google drive.\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                \n",
        "if __name__ == \"__main__\":\n",
        "    file_id = '1ih8PomVE7L3z_xReHAEsq3hk-0O1Uo12'\n",
        "    destination = 'data.csv'\n",
        "    download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKhyLy_67v40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing and cleaning data using pandas library\n",
        "data = pd.read_csv('data.csv')\n",
        "del data['Unnamed: 32']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm1s-RDt7v42",
        "colab_type": "code",
        "outputId": "2218ec0e-199f-4112-8f60-4894a194c033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "## Observe the data\n",
        "data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id diagnosis  ...  symmetry_worst  fractal_dimension_worst\n",
              "0      842302         M  ...          0.4601                  0.11890\n",
              "1      842517         M  ...          0.2750                  0.08902\n",
              "2    84300903         M  ...          0.3613                  0.08758\n",
              "3    84348301         M  ...          0.6638                  0.17300\n",
              "4    84358402         M  ...          0.2364                  0.07678\n",
              "..        ...       ...  ...             ...                      ...\n",
              "564    926424         M  ...          0.2060                  0.07115\n",
              "565    926682         M  ...          0.2572                  0.06637\n",
              "566    926954         M  ...          0.2218                  0.07820\n",
              "567    927241         M  ...          0.4087                  0.12400\n",
              "568     92751         B  ...          0.2871                  0.07039\n",
              "\n",
              "[569 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbJ5Q9bG7v46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We have left first two columns and taken other columns as input features\n",
        "X = data.iloc[:, 2:].values\n",
        "\n",
        "# 2nd column is output labels\n",
        "y = data.iloc[:, 1].values\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWuui_K4NwKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "7e4902d86acc8fa2c5d0f76bd24f32c3",
          "grade": false,
          "grade_id": "cell-01b522c8bd43d71e",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "U88avJVb7v48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Convert the output labels to numbers : M->0; B-> 1\n",
        "## Store the output in Y_v\n",
        "Y_v = []\n",
        "\n",
        "for i in range(len(y)):\n",
        "  if(y[i]=='M'):\n",
        "    Y_v.append(0)\n",
        "  else:\n",
        "    Y_v.append(1)\n",
        "    \n",
        "print(Y_v)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7eK2XCH7v4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "b159f91eea4d5ad096c0fb253da3d8fa",
          "grade": false,
          "grade_id": "cell-c00e0c2ad0883f9f",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "pL2Jndg07v5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### One-hot encode Y_v\n",
        "def oneHot(y, Ny):\n",
        "    '''\n",
        "    Input:\n",
        "        y: an int in {0, 1}\n",
        "        Ny: Number of classes, e.g., 2 here.\n",
        "    Output:\n",
        "        Y: a vector of Ny (=2) tuples\n",
        "    '''\n",
        "    Y=np.zeros(Ny)\n",
        "    if(y==0):\n",
        "      Y[0]=1\n",
        "    else:\n",
        "      Y[1]=1\n",
        "    return Y\n",
        "    \n",
        "    ##Y=keras.utils.to_categorical(y,num_classes=Ny);\n",
        "    #return Y\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "18757997ca217ee5adacc916e0d24de7",
          "grade": false,
          "grade_id": "cell-40a453f8bfe0b014",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "bZRghoyX7v5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "### Split data into train and test. Keep 10% of samples for testing\n",
        "## Divide the data into these variables - X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test=train_test_split(X,Y_v,test_size=0.1,random_state=42)\n",
        "print(type(X_train))\n",
        "print(type(y_train))\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "81a8d49e875948d9258cc8ed13615bfc",
          "grade": false,
          "grade_id": "cell-0b0ff2b9c8bb25c2",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "Os0R-5AR7v5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Normalize the Data\n",
        "def findMeanStddev(X):\n",
        "    '''\n",
        "    Input: \n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "    Output:\n",
        "        mean: mean of samples in X (same size as X)\n",
        "        stddev: element-wise std dev of sample in X (same size as X)\n",
        "    '''\n",
        "    mean=np.mean(X,axis=0)\n",
        "    stddev=np.std(X,axis=0)\n",
        "    return mean,stddev\n",
        "    \n",
        "\n",
        "def normalizeX(X, mean, stddev):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "        mean: mean of samples in X (same size as X)\n",
        "        stddev: element-wise std dev of sample in X (same size as X) \n",
        "    Output:\n",
        "        Xn: X modified to have 0 mean and 1 std dev\n",
        "    '''\n",
        "    Xn=np.divide((X-mean),stddev)\n",
        "    return Xn\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "08580dc7ce1e83bf2126742f4369ac2d",
          "grade": true,
          "grade_id": "cell-0a245e71f4ed1f97",
          "locked": true,
          "points": 2,
          "schema_version": 1,
          "solution": false
        },
        "id": "9u5yU4Oa7v5U",
        "colab_type": "code",
        "outputId": "750cd591-f6c9-4b97-f320-5bc0babe0d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"test for normalizeX\"\"\"\n",
        "#normalization\n",
        "x_tr=[]\n",
        "x_te=[]\n",
        "'''\n",
        "for i in range(len(y_train)):\n",
        "  y_tr.append(oneHot(Y_v[i],2))\n",
        "for i in range(len(y_test)):\n",
        "  y_te.append(oneHot(Y_v[i],2))\n",
        "'''  \n",
        "mean,stddev=findMeanStddev(X_train)\n",
        "x_tr=normalizeX(X_train,mean,stddev)\n",
        "mean,stddev=findMeanStddev(X_test)\n",
        "x_te=normalizeX(X_test,mean,stddev)\n",
        "\n",
        "#conversion of lists into arrays\n",
        "\n",
        "x_tr=np.array(x_tr)\n",
        "x_te=np.array(x_te)\n",
        "y_train=np.array(y_train)\n",
        "y_test=np.array(y_test)\n",
        "print(x_tr.shape)\n",
        "print(np.array(y_train).shape)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(512, 30)\n",
            "(512,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpcwAzUK7v5X",
        "colab_type": "text"
      },
      "source": [
        "#### Create model. \n",
        "- Choose the number of hidden layers, neurons, activations, loss function, learning rate and optimizers on your own.\n",
        "- Report accuracy metric\n",
        "- Use no more than 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "7382f21bea96bee0017462ed4344fad0",
          "grade": false,
          "grade_id": "cell-30d963a10b861f98",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "gd5nBHkI7v5Y",
        "colab_type": "code",
        "outputId": "18f1ff64-f48d-47a3-e3e6-4e9aeb3afef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import metrics\n",
        "#model\n",
        "model=Sequential()\n",
        "\n",
        "#layers\n",
        "model.add(Dense(250,activation='relu',input_shape=(x_tr.shape[1],)))\n",
        "model.add(Dense(250,activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam',loss='mean_squared_error',metrics=[metrics.binary_accuracy])\n",
        "\n",
        "#stop epochs if val_loss doesn't improve\n",
        "from keras.callbacks import EarlyStopping\n",
        "stop_early=EarlyStopping(patience=6)\n",
        "\n",
        "#training\n",
        "model.fit(x_tr,y_train,validation_split=0.2,epochs=30,callbacks=[stop_early])\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 409 samples, validate on 103 samples\n",
            "Epoch 1/30\n",
            "409/409 [==============================] - 0s 656us/step - loss: 0.2353 - binary_accuracy: 0.7311 - val_loss: 0.1145 - val_binary_accuracy: 0.8252\n",
            "Epoch 2/30\n",
            "409/409 [==============================] - 0s 103us/step - loss: 0.0988 - binary_accuracy: 0.8924 - val_loss: 0.0883 - val_binary_accuracy: 0.9126\n",
            "Epoch 3/30\n",
            "409/409 [==============================] - 0s 86us/step - loss: 0.0568 - binary_accuracy: 0.9487 - val_loss: 0.0715 - val_binary_accuracy: 0.9223\n",
            "Epoch 4/30\n",
            "409/409 [==============================] - 0s 87us/step - loss: 0.0413 - binary_accuracy: 0.9682 - val_loss: 0.0666 - val_binary_accuracy: 0.9417\n",
            "Epoch 5/30\n",
            "409/409 [==============================] - 0s 92us/step - loss: 0.0319 - binary_accuracy: 0.9878 - val_loss: 0.0700 - val_binary_accuracy: 0.9320\n",
            "Epoch 6/30\n",
            "409/409 [==============================] - 0s 81us/step - loss: 0.0285 - binary_accuracy: 0.9878 - val_loss: 0.0598 - val_binary_accuracy: 0.9417\n",
            "Epoch 7/30\n",
            "409/409 [==============================] - 0s 94us/step - loss: 0.0223 - binary_accuracy: 0.9951 - val_loss: 0.0523 - val_binary_accuracy: 0.9417\n",
            "Epoch 8/30\n",
            "409/409 [==============================] - 0s 92us/step - loss: 0.0176 - binary_accuracy: 0.9951 - val_loss: 0.0582 - val_binary_accuracy: 0.9417\n",
            "Epoch 9/30\n",
            "409/409 [==============================] - 0s 95us/step - loss: 0.0154 - binary_accuracy: 0.9976 - val_loss: 0.0556 - val_binary_accuracy: 0.9515\n",
            "Epoch 10/30\n",
            "409/409 [==============================] - 0s 92us/step - loss: 0.0130 - binary_accuracy: 0.9951 - val_loss: 0.0527 - val_binary_accuracy: 0.9515\n",
            "Epoch 11/30\n",
            "409/409 [==============================] - 0s 92us/step - loss: 0.0117 - binary_accuracy: 0.9976 - val_loss: 0.0522 - val_binary_accuracy: 0.9417\n",
            "Epoch 12/30\n",
            "409/409 [==============================] - 0s 99us/step - loss: 0.0106 - binary_accuracy: 1.0000 - val_loss: 0.0469 - val_binary_accuracy: 0.9515\n",
            "Epoch 13/30\n",
            "409/409 [==============================] - 0s 106us/step - loss: 0.0097 - binary_accuracy: 1.0000 - val_loss: 0.0500 - val_binary_accuracy: 0.9515\n",
            "Epoch 14/30\n",
            "409/409 [==============================] - 0s 99us/step - loss: 0.0087 - binary_accuracy: 1.0000 - val_loss: 0.0458 - val_binary_accuracy: 0.9515\n",
            "Epoch 15/30\n",
            "409/409 [==============================] - 0s 113us/step - loss: 0.0081 - binary_accuracy: 1.0000 - val_loss: 0.0495 - val_binary_accuracy: 0.9515\n",
            "Epoch 16/30\n",
            "409/409 [==============================] - 0s 98us/step - loss: 0.0073 - binary_accuracy: 1.0000 - val_loss: 0.0466 - val_binary_accuracy: 0.9515\n",
            "Epoch 17/30\n",
            "409/409 [==============================] - 0s 86us/step - loss: 0.0068 - binary_accuracy: 1.0000 - val_loss: 0.0482 - val_binary_accuracy: 0.9417\n",
            "Epoch 18/30\n",
            "409/409 [==============================] - 0s 90us/step - loss: 0.0072 - binary_accuracy: 1.0000 - val_loss: 0.0499 - val_binary_accuracy: 0.9515\n",
            "Epoch 19/30\n",
            "409/409 [==============================] - 0s 92us/step - loss: 0.0058 - binary_accuracy: 1.0000 - val_loss: 0.0482 - val_binary_accuracy: 0.9515\n",
            "Epoch 20/30\n",
            "409/409 [==============================] - 0s 95us/step - loss: 0.0051 - binary_accuracy: 1.0000 - val_loss: 0.0419 - val_binary_accuracy: 0.9515\n",
            "Epoch 21/30\n",
            "409/409 [==============================] - 0s 108us/step - loss: 0.0046 - binary_accuracy: 1.0000 - val_loss: 0.0431 - val_binary_accuracy: 0.9515\n",
            "Epoch 22/30\n",
            "409/409 [==============================] - 0s 106us/step - loss: 0.0037 - binary_accuracy: 1.0000 - val_loss: 0.0466 - val_binary_accuracy: 0.9515\n",
            "Epoch 23/30\n",
            "409/409 [==============================] - 0s 101us/step - loss: 0.0035 - binary_accuracy: 1.0000 - val_loss: 0.0453 - val_binary_accuracy: 0.9515\n",
            "Epoch 24/30\n",
            "409/409 [==============================] - 0s 99us/step - loss: 0.0033 - binary_accuracy: 1.0000 - val_loss: 0.0431 - val_binary_accuracy: 0.9515\n",
            "Epoch 25/30\n",
            "409/409 [==============================] - 0s 96us/step - loss: 0.0031 - binary_accuracy: 1.0000 - val_loss: 0.0442 - val_binary_accuracy: 0.9515\n",
            "Epoch 26/30\n",
            "409/409 [==============================] - 0s 97us/step - loss: 0.0028 - binary_accuracy: 1.0000 - val_loss: 0.0421 - val_binary_accuracy: 0.9515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcc0efa3828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b747269f44ed2955cd785237259aa5ac",
          "grade": true,
          "grade_id": "cell-6013da447b543c56",
          "locked": true,
          "points": 2,
          "schema_version": 1,
          "solution": false
        },
        "id": "3maBYRaa7v5c",
        "colab_type": "code",
        "outputId": "6591b311-b679-4979-d9b1-7baeb7ce4a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"Test for model\"\"\"\n",
        "'''\n",
        "from sklearn.metrics import confusion_matrix\n",
        "pred=model.predict(x_te)\n",
        "count=0\n",
        "confusionMatrix=[]\n",
        "confusionMatrix=confusion_matrix(y_test,pred)\n",
        "n=confusionMatrix.shape[0]\n",
        "for i in range(n):\n",
        "  count=count+confusionMatrix[i][i]\n",
        "\n",
        "#accuracy\n",
        "accuracy=count/(n*n)\n",
        "print(accuracy)\n",
        "'''\n",
        "\n",
        "accuracy=model.evaluate(x_te,y_test)\n",
        "print('Accuracy:',accuracy[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 0s 81us/step\n",
            "Accuracy: 0.9649122807017544\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}